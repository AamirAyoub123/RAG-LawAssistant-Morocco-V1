# backend/scripts/ingest_data_improved.py
import os
import re
from typing import Dict, List
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer

def semantic_chunk_text(text: str, max_chunk_size: int = 800) -> List[str]:
    """D√©coupe le texte en chunks s√©mantiques"""
    chunks = []
    
    # 1. D'abord diviser par articles 
    articles = re.split(r'(?=Article\s+\d+[\.\s\-])', text)
    
    for article in articles:
        article = article.strip()
        if not article or len(article) < 50:
            continue
        
        # Si l'article est raisonnable, le garder entier
        if len(article) <= max_chunk_size:
            chunks.append(article)
        else:
            # Sinon, diviser par paragraphes naturels
            paragraphs = re.split(r'\n\s*\n', article)
            current_chunk = ""
            
            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue
                
                if len(current_chunk) + len(para) <= max_chunk_size:
                    current_chunk += para + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
    
    # 2. Si pas d'articles, diviser par paragraphes
    if not chunks:
        paragraphs = re.split(r'\n\s*\n', text)
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(current_chunk) + len(para) <= max_chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
    
    # Filtrer les chunks trop courts
    chunks = [chunk for chunk in chunks if len(chunk) >= 100]
    
    return chunks

def extract_metadata(text: str) -> Dict:
    """Extrait des m√©tadonn√©es du texte"""
    metadata = {
        'has_articles': bool(re.search(r'Article\s+\d+', text)),
        'article_numbers': re.findall(r'Article\s+(\d+)', text),
        'has_decret': bool(re.search(r'D√©cret\s+n¬∞', text, re.IGNORECASE)),
        'has_arrete': bool(re.search(r'Arr√™t√©', text, re.IGNORECASE)),
        'has_loi': bool(re.search(r'Loi\s+n¬∞', text, re.IGNORECASE)),
        'word_count': len(text.split()),
        'contains_dates': bool(re.search(r'\d{1,2}\s+\w+\s+\d{4}', text)),
    }
    return metadata

def ingest_with_metadata():
    """Ing√®re les documents avec m√©tadonn√©es enrichies"""
    client = QdrantClient(url="http://localhost:6333")
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    
    # Cr√©er ou r√©initialiser la collection
    try:
        client.delete_collection("moroccan_law")
        print("üóëÔ∏è  Ancienne collection supprim√©e")
    except:
        pass
    
    client.create_collection(
        collection_name="moroccan_law",
        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)
    )
    
    print("üîÑ INGESTION AVEC M√âTADONN√âES")
    print("=" * 60)
    
    data_folder = "E:/moroccan-law-rag-v1/data/cleaned"
    point_id = 0
    
    for filename in os.listdir(data_folder):
        if filename.endswith('.txt'):
            filepath = os.path.join(data_folder, filename)
            print(f"\nüìñ {filename}")
            
            with open(filepath, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # Cr√©er des chunks s√©mantiques
            chunks = semantic_chunk_text(text)
            print(f"   üì¶ Chunks s√©mantiques: {len(chunks)}")
            
            # Traiter chaque chunk
            for i, chunk in enumerate(chunks):
                # Extraire les m√©tadonn√©es
                metadata = extract_metadata(chunk)
                
                # Cr√©er l'embedding
                embedding = embedder.encode([chunk])[0].tolist()
                
                # Pr√©parer le point
                point = {
                    "id": point_id,
                    "vector": embedding,
                    "payload": {
                        "text": chunk,
                        "filename": filename,
                        "chunk_id": i,
                        "chunk_count": len(chunks),
                        **metadata, 
                        "is_legal_document": any([
                            metadata['has_articles'],
                            metadata['has_decret'],
                            metadata['has_arrete'],
                            metadata['has_loi']
                        ])
                    }
                }
                client.upsert(collection_name="moroccan_law", points=[point])
                point_id += 1
            
            print(f"   ‚úÖ {len(chunks)} chunks upload√©s")
    
    collection_info = client.get_collection("moroccan_law")
    print(f"\nüéØ INGESTION TERMIN√âE!")
    print(f"üìä Points totaux: {collection_info.points_count}")

if __name__ == "__main__":
    ingest_with_metadata()